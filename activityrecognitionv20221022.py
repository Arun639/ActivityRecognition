{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n\ndftrain = pd.read_csv('../input/activityrecognitionmdarunkumar/train.csv')\nprint(dftrain.head())\n\n\nnp.unique(dftrain.Activity.values)\n\n\ndef mappingDictionary(df,col):\n    unqDict = np.unique(df[col])\n    dct = dict()\n    for i,x in enumerate(unqDict):\n        dct.update({x:i})\n    return dct\n\nmappedActivities = mappingDictionary(dftrain,'Activity')\ndftrain['Activity'] = dftrain['Activity'].map(mappedActivities)\n\n\nnpTrain = dftrain.values\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dense,Flatten,Dropout,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n\nmodel = Sequential()\nmodel.add(Dense(512,activation = 'relu',kernel_regularizer = 'l1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256,activation = 'relu',kernel_regularizer = 'l1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64,activation = 'relu',kernel_regularizer = 'l1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,activation = 'relu',kernel_regularizer = 'l1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16,activation = 'relu',kernel_regularizer = 'l1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6,activation = 'softmax',kernel_regularizer = 'l1'))\nmodel.compile(loss = 'categorical_crossentropy',optimizer = Adam(learning_rate = 2e-4),metrics = ['acc'])\ncheck_point = ModelCheckpoint(filepath = 'Checkpointmodelv1.h5',\n                                         monitor = 'val_loss',\n                                         mode = 'min',\n                                         save_best_only=True)\nearlyStopping = EarlyStopping(monitor = 'val_loss',patience =20,mode = 'min',verbose = 1)\nplateauDecay = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0,\n                                 mode='min')\ncallback_list=[check_point,earlyStopping,plateauDecay]\n\nnpTrain.shape\n\nfrom sklearn.decomposition import PCA\ninpData = npTrain\nkernelNeurons = [512,256,64,32,16]\nfor indx in range(5):\n    pca = PCA(n_components = kernelNeurons[indx])\n    pca.fit(inpData)\n    kernelComp = pca.components_.T\n    model.layers[indx].set_weights = kernelComp\n    inpData = np.dot(inpData,kernelComp)\npca = PCA(n_components = 512)\nnpTrainPCA = pca.fit_transform(npTrain[:,:-1])\n\nxtrain = npTrainPCA\nytrain = np.zeros(shape = (npTrain[:,-1].shape[0],6),dtype = np.int32)\nfor i,x in enumerate(dftrain.iloc[:,-1]):\n    ytrain[i,x] = 1\n\nfrom sklearn.model_selection import train_test_split\nXTrain,XVal,yTrain,yVal = train_test_split(xtrain,ytrain,test_size = 0.33,random_state = 0)\n\nhisty = model.fit(x = XTrain,y = yTrain,validation_data = (XVal,yVal),\n                  batch_size = 64,epochs = 300,validation_steps = 16)\n\nmodel.summary()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nepochs = range(1,301)\nplt.plot(epochs, histy.history['loss'], label='Training Loss')\nplt.plot(epochs, histy.history['val_loss'], label='Validation Loss')\n \n# Add in a title and axes labels\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n \n# Display the plot\nplt.legend(loc='best')\nplt.show()\n\nplt.plot(epochs, histy.history['acc'], label='Training acc')\nplt.plot(epochs, histy.history['val_acc'], label='Validation acc')\n \n# Add in a title and axes labels\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n \n# Display the plot\nplt.legend(loc='best')\nplt.show()\n\n# %% [markdown]\n# **Test Data Forecasting**\n\ndfTest = pd.read_csv('../input/activityrecognitionmdarunkumar/test.csv')\ndfTest['Activity'] = dfTest['Activity'].map(mappedActivities)\nxTest = dfTest.values[:,:-1]\nyTest = dfTest.values[:,-1]\n\nxTest = pca.transform(xTest)\nyTestEnc = np.zeros(shape = (dfTest.shape[0],6),dtype = np.float32)\n\nfor i,x in enumerate(yTest):\n    yTestEnc[i,int(x)] = 1\nyTest = yTestEnc\n\nyPred = model.predict(xTest)\n\nimport tensorflow as tf\nyTestLabels = tf.argmax(yTest, axis = 1)\nyPredLabels = tf.argmax(yPred, axis = 1)\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(yTestLabels,yPredLabels))\nprint(accuracy_score(yTestLabels,yPredLabels))\n\n\ncm = confusion_matrix(yTestLabels,yPredLabels)\nsns.heatmap(cm,xticklabels = mappedActivities.keys(),yticklabels =  mappedActivities.keys())","metadata":{"_uuid":"c93b3bad-eb8a-463f-998e-02dfb414446e","_cell_guid":"9c10431c-ccf2-4e2c-a636-c88e8af7254a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}